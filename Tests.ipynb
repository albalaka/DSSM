{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = 28\n",
    "test_range = 7\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_l = 31 # seasonal model, 24 hours + 7 days\n",
    "dim_z = 1\n",
    "initial_variance = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_z = np.load('data/formatted_traffic.npy')\n",
    "train_z = np.reshape(all_z[:,:train_range,:], [all_z.shape[0], -1])\n",
    "test_z = np.reshape(all_z[:,train_range:,:], [all_z.shape[0], -1])\n",
    "del all_z\n",
    "train_x = np.load('data/train_features.npy')\n",
    "test_x = np.load('data/test_features.npy')\n",
    "train_x = np.transpose(train_x, (1,0,2))\n",
    "test_x = np.transpose(test_x, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_z.shape)\n",
    "print(test_z.shape)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "def fn(params, inputs):\n",
    "    a = params\n",
    "    b = inputs\n",
    "\n",
    "    return tf.add(a,b)\n",
    "\n",
    "inputs = np.array([x for x in range(27)]).reshape([9,3]).astype(np.float32)\n",
    "print(inputs.shape)\n",
    "inputs = tf.tile(tf.expand_dims(tf.convert_to_tensor(inputs),0), (4,1,1))\n",
    "dummy_a = tf.ones([4,3])\n",
    "print(inputs.shape)\n",
    "forward_states = tf.scan(fn, tf.transpose(inputs, [1,0,2]),\n",
    "                        initializer = (dummy_a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tf.expand_dims(inputs,-1).shape)\n",
    "print(dummy_a.shape)\n",
    "print(forward_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    dim_l = 2\n",
    "    dim_z = 1\n",
    "    sample_len = 25\n",
    "    \n",
    "    z = np.array([x+np.random.randn() for x in range(sample_len)]).astype(np.float32)\n",
    "    z = tf.expand_dims(tf.expand_dims(tf.convert_to_tensor(z), -1),0)\n",
    "    l_0 = tf.expand_dims(tf.expand_dims(tf.constant([0,0], dtype=tf.float32), -1), 0)\n",
    "    P_0 = tf.expand_dims(tf.constant([[5,5],[5,5]], dtype = tf.float32), 0)\n",
    "    F = tf.expand_dims(tf.tile(tf.expand_dims(tf.convert_to_tensor(np.array([[1,0],[0,1]], dtype = np.float32)), 0), (sample_len,1,1)),0)\n",
    "    Q = tf.expand_dims(tf.random.normal((sample_len,dim_l, dim_l)), 0)\n",
    "    R = tf.expand_dims(tf.random.normal((sample_len, dim_z, dim_z)),0)\n",
    "    a = tf.expand_dims(tf.ones((sample_len, dim_l, dim_z)),0)\n",
    "    b = tf.expand_dims(tf.ones((sample_len,dim_z)),0)\n",
    "\n",
    "    kf = KalmanFilter(dim_l=dim_l, dim_z=dim_z, l_0=l_0, P_0=P_0, F=F, a=a, b=b, Q=Q, R=R, z=z)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    l_filtered, P_filtered = kf.filter()\n",
    "    a = tf.squeeze(a,0)\n",
    "    b = tf.squeeze(b,0)\n",
    "    l_filtered = tf.squeeze(l_filtered, 0)\n",
    "    preds = []\n",
    "    for i in range(z.shape[1]):\n",
    "        preds.append((tf.matmul(a[i],l_filtered[i], transpose_a=True)+b[i]).eval())\n",
    "    preds = np.concatenate(np.concatenate(preds))\n",
    "    plt.plot(preds)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim_l = 2\n",
    "dim_z = 1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "z = np.array([x for x in range(27)]).astype(np.float32)\n",
    "z = tf.expand_dims(tf.expand_dims(tf.convert_to_tensor(z), -1),0)\n",
    "l_0 = tf.expand_dims(tf.expand_dims(tf.constant([0,0], dtype=tf.float32), -1), 0)\n",
    "P_0 = tf.expand_dims(tf.constant([[5,5],[5,5]], dtype = tf.float32), 0)\n",
    "F = tf.expand_dims(tf.tile(tf.expand_dims(tf.convert_to_tensor(np.array([[1,0],[0,1]], dtype = np.float32)), 0), (27,1,1)),0)\n",
    "Q = tf.expand_dims(tf.random.normal((27,dim_l, dim_l)), 0)\n",
    "R = tf.expand_dims(tf.random.normal((27, dim_z, dim_z)),0)\n",
    "a = tf.expand_dims(tf.ones((27, dim_l, dim_z)),0)\n",
    "b = tf.expand_dims(tf.ones((27,dim_z)),0)\n",
    "\n",
    "kf = KalmanFilter(dim_l=dim_l, dim_z=dim_z, l_0=l_0, P_0=P_0, F=F, a=a, b=b, Q=Q, R=R, z=z)\n",
    "l_filtered, P_filtered = kf.filter()\n",
    "a = tf.squeeze(a,0)\n",
    "b = tf.squeeze(b,0)\n",
    "l_filtered = tf.squeeze(l_filtered, 0)\n",
    "preds = []\n",
    "for i in range(z.shape[1]):\n",
    "    preds.append(tf.matmul(a[i],l_filtered[i], transpose_a=True)+b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KalmanFilter(dim_l=dim_l, dim_z=dim_z, l_0=l_0, P_0=P_0, F=F, a=a, b=b, Q=Q, R=R, z=z)\n",
    "l_filtered, P_filtered = kf.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing conversion of LSTM output to SSM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter(object):\n",
    "    \"\"\"\n",
    "    This class defines a kalman filter\n",
    "\n",
    "    l - latent state\n",
    "    l_a_priori - A priori state estimate\n",
    "    l_a_posteriori - A posteriori state estimate\n",
    "\n",
    "    P_a_priori - A priori error covariance\n",
    "    P_a_posteriori - A posteriori error covariance\n",
    "\n",
    "    F - state-transition model\n",
    "    Q - covariance of the process noise    \n",
    "    a, b - observation model and bias\n",
    "    R - covariance of the observation noise\n",
    "    z - observation\n",
    "\n",
    "    y_pre - measurement pre-fit residual\n",
    "    S - Pre-fit residual covariance\n",
    "    K - Kalman gain\n",
    "    y_post - measurement post-fit residual\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_l, dim_z, **kwargs):\n",
    "        self.dim_l = dim_l\n",
    "        self.dim_z = dim_z\n",
    "        self.dim_y = dim_z\n",
    "\n",
    "        # lambda initializer for identity matrices\n",
    "        self.eye_init = lambda shape, dtype = np.float32: np.eye(*shape, dtype = dtype)\n",
    "\n",
    "        self._I = tf.constant(self.eye_init((dim_l, dim_l)), name= 'I')\n",
    "\n",
    "        # Pop all variables\n",
    "        '''This section requires these kwargs to exist, cannot handle missing args'''\n",
    "        with tf.variable_scope('KF', reuse = tf.AUTO_REUSE):\n",
    "            init = kwargs.pop('l_0', np.zeros((dim_l, ), dtype=np.float32))\n",
    "            self.l_0 = tf.get_variable('l_0', initializer=init, trainable=False)  # Predicted state estimate\n",
    "\n",
    "            init = kwargs.pop('P_0', self.eye_init((dim_l, dim_l)))\n",
    "            self.P_0 = tf.get_variable('P_0', initializer=init, trainable=False)  # Predicted error covariance\n",
    "\n",
    "            init = kwargs.pop('F', self.eye_init((dim_l, dim_l)))\n",
    "            self.F = tf.get_variable('F', initializer = init)\n",
    "\n",
    "            init = kwargs.pop('Q', self.eye_init((dim_l, dim_l)))\n",
    "            self.Q = tf.get_variable('Q', initializer = init, trainable = False)\n",
    "\n",
    "            init = kwargs.pop('a', np.zeros((dim_l,), dtype=np.float32))\n",
    "            self.a = tf.get_variable('a', initializer=init)\n",
    "\n",
    "            init = kwargs.pop('b', np.zeros((dim_z,), dtype=np.float32))\n",
    "            self.b = tf.get_variable('b', initializer=init)\n",
    "\n",
    "            init = kwargs.pop('R', self.eye_init((dim_z, dim_z)))\n",
    "            self.R = tf.get_variable('R', initializer=init, trainable = False)\n",
    "\n",
    "            init = kwargs.pop('y_0', np.zeros((self.dim_y), dtype = np.float32))\n",
    "            self.y_0 = tf.get_variable('y_0', initializer = init) # initial prediction\n",
    "\n",
    "#         print('l_0', self.l_0)\n",
    "#         print('P_0', self.P_0)\n",
    "#         print('F', self.F)\n",
    "#         print('Q', self.Q)\n",
    "#         print('a', self.a)\n",
    "#         print('b', self.b)\n",
    "#         print('R', self.R)\n",
    "\n",
    "        '''This section can handle missing kwargs'''\n",
    "        self.z = kwargs.pop('z', None)\n",
    "        if self.z is None:\n",
    "            self.z = tf.placeholder(tf.float32, shape = [None, None, dim_z], name = 'z')\n",
    "#         print('z', self.z)\n",
    "\n",
    "    def forward_step_fn(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Forward step over a batch\n",
    "        params contains: l_a_posteriori, P_a_posteriori, TODO: What else??\n",
    "        inputs contains: batch_size, variable dimensions, TODO: Need this??\n",
    "\n",
    "        Calculates prior distributions based on the given posterior distributions and the current residual\n",
    "                updates posterior distributions based on the new prior distributions\n",
    "        \"\"\"\n",
    "        '''Shapes:\n",
    "            z = (bs, dim_z)\n",
    "            l_a_posteriori = (bs, dim_l, dim_z)\n",
    "            P_a_posteriori = (bs, dim_l, dim_l)\n",
    "            F = (bs, dim_l, dim_l)\n",
    "            Q = (bs, dim_l, dim_l)\n",
    "            R = (bs, dim_z, dim_z)\n",
    "            a = (bs, dim_l, dim_z)\n",
    "            b = (bs, dim_z)\n",
    "        '''\n",
    "        \n",
    "        z, F, Q, R, a, b = inputs\n",
    "        l_a_posteriori, P_a_posteriori, y_pre = params\n",
    "\n",
    "#         print('z',z)\n",
    "#         print('F', F)\n",
    "#         print('Q', Q)\n",
    "#         print('R', R)\n",
    "#         print('a', a)\n",
    "#         print('b', b)\n",
    "#         print('l', l_a_posteriori)\n",
    "#         print('P', P_a_posteriori)\n",
    "#         print('y_pre', y_pre)\n",
    "        \n",
    "        l_a_priori = tf.matmul(F,l_a_posteriori)\n",
    "        P_a_priori = tf.matmul(tf.matmul(F,P_a_posteriori), F, transpose_b = True) + Q\n",
    "        \n",
    "        y_pre = tf.expand_dims(z - tf.add(tf.squeeze(tf.matmul(a, l_a_priori, transpose_a=True),-1), b),-1)\n",
    "#         print('y_pre', y_pre)\n",
    "        S = R + tf.matmul(tf.matmul(a, P_a_priori, transpose_a=True), a)\n",
    "#         print('S', S)\n",
    "#         S_inv = tf.matrix_inverse(S)\n",
    "        S_inv = tf.reciprocal(S)\n",
    "        K = tf.matmul(tf.matmul(P_a_priori, a), S_inv)\n",
    "        \n",
    "        l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "\n",
    "        I_Ka = self._I-tf.matmul(K,a, transpose_b=True)\n",
    "        P_a_posteriori = tf.matmul(tf.matmul(I_Ka, P_a_priori), I_Ka, transpose_b=True) + \\\n",
    "                         tf.matmul(tf.matmul(K,R), K, transpose_b=True)\n",
    "        \n",
    "        y_post = z - tf.add(tf.squeeze(tf.matmul(a, l_a_posteriori, transpose_a=True),-1), b)\n",
    "        return l_a_posteriori, P_a_posteriori, y_post\n",
    "\n",
    "    def compute_forwards(self):\n",
    "        \"\"\"\n",
    "        Compute the forward step in Kalman Filter\n",
    "        The forward pass is initialized with p(x_1) = N(self.x, self.P)\n",
    "        We return the mean and covariance for p(x_t|x_tm1) for t=2, ..., T+1\n",
    "        and the filtering distribution p(x_t|z_1:t) for t=1, ..., T\n",
    "        \"\"\"\n",
    "        def trans(tensor):\n",
    "            if len(tensor.shape)==3:\n",
    "                return tf.transpose(tensor, [1,0,2])\n",
    "            else:\n",
    "                return tf.transpose(tensor, [1,0,2,3])\n",
    "            \n",
    "        forward_states = tf.scan(self.forward_step_fn,\n",
    "                                 elems = (trans(self.z),trans(self.F),\n",
    "                                          trans(self.Q),trans(self.R),\n",
    "                                          trans(self.a),trans(self.b)),\n",
    "                                initializer=(self.l_0, self.P_0, self.y_0))\n",
    "        \n",
    "        return forward_states\n",
    "    \n",
    "    def Kfilter(self):\n",
    "        l_filtered, P_filtered, y_preds = self.compute_forwards()\n",
    "        return (tf.transpose(l_filtered, [1,0,2,3]),\n",
    "                tf.transpose(P_filtered, [1,0,2,3]),\n",
    "                tf.transpose(y_preds, [1,0,2]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_SSM_model(object):\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        self.saved_model_location = './tmp/LSTM_SSM_model.ckpt'\n",
    "#         self.saved_model_location=''\n",
    "        self.saver = None\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.train_range = 28\n",
    "        num_samples, sample_len, feature_len = (963,672,994)\n",
    "        \n",
    "        self.lstm_sizes = [128,64]\n",
    "        last_lstm = self.lstm_sizes[-1]\n",
    "        \n",
    "        self.dim_l = 31 # seasonal model, 24 hours + 7 days\n",
    "        self.dim_z = 1\n",
    "        self.initial_variance = 20\n",
    "        \n",
    "        self.F = tf.tile(tf.expand_dims(tf.expand_dims(tf.eye(self.dim_l),0),0),\n",
    "                         (self.batch_size,sample_len,1,1))\n",
    "        with tf.variable_scope('KF', reuse = tf.AUTO_REUSE):\n",
    "            # Initialize with random values\n",
    "            #self.a = tf.placeholder(tf.float32, shape = [None, None, self.dim_l, self.dim_z], name = 'a')\n",
    "            self.a = tf.get_variable(initializer = tf.ones([self.batch_size, sample_len, self.dim_l, self.dim_z]),\n",
    "                                     dtype = tf.float32, name = 'a')\n",
    "            self.W_a = tf.get_variable(initializer = tf.random.normal([self.batch_size, sample_len, self.dim_l, last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_a') # dim_l, lstm_outputs.shape[-1]\n",
    "            self.bias_a = tf.get_variable(initializer = tf.zeros([self.batch_size, sample_len, self.dim_l, self.dim_z]),\n",
    "                                          dtype = tf.float32, name = 'bias_a')\n",
    "\n",
    "            # Initialize with random values\n",
    "            #self.b = tf.placeholder(tf.float32, shape = [None, None, self.dim_z], name = 'b')\n",
    "            self.b = tf.get_variable(initializer = tf.ones([self.batch_size, sample_len, self.dim_z]),\n",
    "                                    dtype = tf.float32, name = 'b')\n",
    "            self.W_b = tf.get_variable(initializer = tf.random.normal([self.batch_size, sample_len,self.dim_z, last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_b') # dim_z, lstm_outputs.shape[-1]\n",
    "            self.bias_b = tf.get_variable(initializer = tf.zeros([self.batch_size, sample_len, self.dim_z]),\n",
    "                                          dtype = tf.float32, name = 'bias_b')\n",
    "\n",
    "            #self.Q = tf.placeholder(tf.float32, shape = [None, None, self.dim_l, self.dim_l], name = 'Q')\n",
    "            self.Q = tf.get_variable(initializer = tf.zeros([self.batch_size, sample_len, self.dim_l, self.dim_l]),\n",
    "                                     dtype = tf.float32, name = 'Q')\n",
    "            self.W_Q = tf.get_variable(initializer =tf.random.normal([self.batch_size, sample_len, self.dim_l, last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_Q')\n",
    "            self.bias_Q = tf.get_variable(initializer = tf.zeros([self.batch_size, sample_len, self.dim_l]),\n",
    "                                          dtype = tf.float32, name = 'bias_Q')\n",
    "\n",
    "            #self.R = tf.placeholder(tf.float32, shape = [None, None, self.dim_z], name = 'R')\n",
    "            self.R = tf.get_variable(initializer = tf.zeros([self.batch_size, sample_len, self.dim_z, self.dim_z]),\n",
    "                                     dtype = tf.float32, name = 'R')\n",
    "            self.W_R = tf.get_variable(initializer = tf.random.normal([self.batch_size, sample_len, self.dim_z, last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_R')\n",
    "            self.bias_R = tf.get_variable(initializer = tf.zeros([self.batch_size, sample_len, self.dim_z]),\n",
    "                                          dtype = tf.float32, name = 'bias_R')\n",
    "        \n",
    "        self.l_0 = tf.zeros([self.batch_size, self.dim_l, self.dim_z], dtype = tf.float32)\n",
    "        self.P_0 = tf.tile(tf.expand_dims(self.initial_variance*tf.eye(self.dim_l, dtype = tf.float32),0),\n",
    "                           (self.batch_size, 1, 1))\n",
    "        self.y_0 = tf.zeros([self.batch_size, self.dim_z], dtype = tf.float32)\n",
    "        \n",
    "        self.lstm_input = tf.placeholder(tf.float32, shape= [None, sample_len, feature_len], name = 'lstm_input')\n",
    "        self.lstm_output = tf.get_variable(initializer = tf.zeros([self.batch_size, sample_len, last_lstm]),\n",
    "                                           dtype = tf.float32, name = 'lstm_output')\n",
    "        self.z = tf.placeholder(tf.float32, shape = [None, sample_len, self.dim_z], name = 'z')\n",
    "        \n",
    "        self.losses = []\n",
    "\n",
    "    def load_data(self):\n",
    "        print(\"Loading Data...\")\n",
    "        # load data\n",
    "        all_z = np.load('data/formatted_traffic.npy')\n",
    "        self.train_z = np.reshape(all_z[:,:self.train_range,:], [all_z.shape[0], -1, 1])\n",
    "        self.test_z = np.reshape(all_z[:,self.train_range:,:], [all_z.shape[0], -1])\n",
    "        del all_z\n",
    "        train_x = np.load('data/train_features.npy')\n",
    "        test_x = np.load('data/test_features.npy')\n",
    "        self.train_x = np.transpose(train_x, (1,0,2))\n",
    "        self.test_x = np.transpose(test_x, (1,0,2))\n",
    "        print(\"...Data Loaded\")\n",
    "        return self\n",
    "        \n",
    "    def build_LSTM(self):\n",
    "        with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            lstms = [tf.contrib.rnn.LSTMCell(size) for size in self.lstm_sizes]\n",
    "            dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.8) for lstm in lstms]\n",
    "\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "            initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "            self.lstm_output, final_state = tf.nn.dynamic_rnn(cell, self.lstm_input, initial_state = initial_state)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def affine_transformations(self):\n",
    "#         with tf.variable_scope('affine_transformations'):\n",
    "            \n",
    "        self.lstm_output = tf.expand_dims(self.lstm_output, -1)\n",
    "\n",
    "        self.a = tf.expand_dims(tf.squeeze(tf.matmul(self.W_a, self.lstm_output)),-1)\n",
    "        self.a = self.a + self.bias_a\n",
    "\n",
    "        self.b = tf.expand_dims(tf.squeeze(tf.matmul(self.W_b, self.lstm_output)),-1)\n",
    "        self.b = self.b + self.bias_b\n",
    "\n",
    "        transition_error = tf.squeeze(tf.matmul(self.W_Q, self.lstm_output))\n",
    "        transition_error = transition_error + self.bias_Q\n",
    "        self.Q = tf.linalg.diag(transition_error)\n",
    "\n",
    "        observation_error = tf.expand_dims(tf.squeeze(tf.matmul(self.W_R, self.lstm_output)),-1)\n",
    "        observation_error = observation_error + self.bias_R\n",
    "        self.R = tf.linalg.diag(observation_error)\n",
    "        return self\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.kf = KalmanFilter(dim_l=self.dim_l,\n",
    "                               dim_z=self.dim_z,\n",
    "                               l_0 = self.l_0,\n",
    "                               P_0 = self.P_0,\n",
    "                               F = self.F,\n",
    "                               Q = self.Q,\n",
    "                               a = self.a,\n",
    "                               b = self.b,\n",
    "                               R = self.R,\n",
    "                               z = self.z,\n",
    "                               y_0 = self.y_0\n",
    "                              )\n",
    "        with tf.variable_scope('Kalman_Filter'):\n",
    "            self.l_filtered, self.P_filtered, self.y_preds = self.kf.Kfilter()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def build_loss(self):\n",
    "        # some series of equations\n",
    "        '''Useful shapes(Ideally):\n",
    "            l_a_posteriori(batch) - (batch_size, sample_len, dim_l)\n",
    "            P_a_posteriori(batch) - (batch_size, sample_len, dim_l,dim_l)\n",
    "            \n",
    "            inputs:\n",
    "                mu_0, a, F, l_a_posteriori?\n",
    "                Sigma_0, a, R, F, P_a_posteriori, Q\n",
    "        '''\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.losses.mean_squared_error(self.z, self.y_preds)\n",
    "        \n",
    "        # What is the tensorflow method of recursively adding values to tensor?\n",
    "        # Iterate through range(sample_len), apply functions, then tf.stack() the elements together in the end?\n",
    "        # tf.scan() ??\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.loss = tf.losses.mean_squared_error(self.z, self.predictions)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        return self\n",
    "    \n",
    "    def initialize_variables(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        if self.saved_model_location is not '' and os.path.exists(self.saved_model_location):\n",
    "            print(\"Restoring model from {}\".format(self.saved_model_location))\n",
    "            self.saver.restore(self.sess, self.saved_model_location)\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        return self\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        start = time.time()\n",
    "        for i in range(epochs):\n",
    "            epoch_loss = []\n",
    "            perm = np.random.permutation(self.train_x.shape[0])\n",
    "            for idx in range((self.train_x.shape[0]//self.batch_size)):\n",
    "                slc = np.array([perm[i] for i in range(idx*32, (idx+1)*32)])\n",
    "                feed_dict = {self.lstm_input: self.train_x[slc],\n",
    "                            self.z: self.train_z[slc]}\n",
    "                loss_, _ = self.sess.run([self.loss, self.optimizer], feed_dict=feed_dict)\n",
    "                epoch_loss.append(loss_)\n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            self.losses.append(epoch_loss)\n",
    "            print(\"Epoch #{}\\tTime Elapsed: {}\\tLoss {}\".format(i, (time.time()-start)/60, epoch_loss))\n",
    "        \n",
    "        self.saver.save(self.sess, self.saved_model_location)\n",
    "        print(\"Model Saved\")\n",
    "        return self.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "...Data Loaded\n",
      "Epoch #0\tTime Elapsed: 1.0416364590326945\tLoss nan\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def initialize_uninitialized(sess):\n",
    "    global_vars          = tf.global_variables()\n",
    "    is_not_initialized   = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    print([str(i.name) for i in not_initialized_vars]) # only for testing\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = LSTM_SSM_model(sess)\n",
    "    model.build_LSTM().affine_transformations().build_model().build_loss().load_data()\n",
    "#     initialize_uninitialized(sess)\n",
    "    model.initialize_variables()\n",
    "    model.train(epochs = 1)\n",
    "#     feed_dict = {model.lstm_input:model.train_x[:32],\n",
    "#                 model.z: model.train_z[:32]}\n",
    "#     preds = model.sess.run([model.y_preds], feed_dict=feed_dict)\n",
    "#     preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working example of just LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm_inputs: (batch_size, 672, 994)\n",
    "\n",
    "lstm_outputs: (batch_size, 672, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(object):\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        self.saved_model_location = './tmp/LSTM_model.ckpt'\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.train_range = 28\n",
    "        train_x_shape = [963,672,994]\n",
    "        train_z_shape = [963,672]\n",
    "    \n",
    "        self.lstm_input = tf.placeholder(tf.float32, shape= [None, train_x_shape[1], train_x_shape[2]])\n",
    "        self.lstm_output = tf.placeholder(tf.float32, shape = [None, train_x_shape[1]])\n",
    "        self.labels_ = tf.placeholder(tf.float32, shape = [None, train_z_shape[1]])\n",
    "        \n",
    "        self.saver = None\n",
    "        self.losses = []\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        print(\"Loading data...\")\n",
    "        # load data\n",
    "        all_z = np.load('data/formatted_traffic.npy')\n",
    "        self.train_z = np.reshape(all_z[:,:self.train_range,:], [all_z.shape[0], -1])\n",
    "        self.test_z = np.reshape(all_z[:,self.train_range:,:], [all_z.shape[0], -1])\n",
    "        del all_z\n",
    "        train_x = np.load('data/train_features.npy')\n",
    "        test_x = np.load('data/test_features.npy')\n",
    "        self.train_x = np.transpose(train_x, (1,0,2))\n",
    "        self.test_x = np.transpose(test_x, (1,0,2))\n",
    "        print(\"...Data loaded\")\n",
    "        return self\n",
    "    \n",
    "    def build_LSTM(self):\n",
    "        lstms = [tf.contrib.rnn.LSTMCell(size, reuse = tf.AUTO_REUSE) for size in [128,64]]\n",
    "        dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.8) for lstm in lstms]\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "        initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "        self.lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cell, self.lstm_input, initial_state = initial_state)\n",
    "\n",
    "        self.predictions = tf.squeeze(tf.contrib.layers.fully_connected(self.lstm_outputs, 1, activation_fn=tf.sigmoid))\n",
    "        return self\n",
    "    \n",
    "    def build_loss(self):\n",
    "        self.loss = tf.losses.mean_squared_error(self.labels_, self.predictions)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        return self\n",
    "    \n",
    "    def initialize_variables(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        if self.saved_model_location is not '' and os.path.exists(self.saved_model_location):\n",
    "            print(\"Restoring model from {}\".format(self.saved_model_location))\n",
    "            self.saver.restore(self.sess, self.saved_model_location)\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        start = time.time()\n",
    "        for i in range(epochs):\n",
    "            epoch_loss = []\n",
    "            perm = np.random.permutation(self.train_x.shape[0])\n",
    "            for idx in range((self.train_x.shape[0]//self.batch_size)):\n",
    "                slc = np.array([perm[i] for i in range(idx*32, (idx+1)*32)])\n",
    "                feed_dict = {self.lstm_input: self.train_x[slc],\n",
    "                            self.labels_: self.train_z[slc]}\n",
    "                loss_, _ = self.sess.run([self.loss, self.optimizer], feed_dict=feed_dict)\n",
    "                epoch_loss.append(loss_)\n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            self.losses.append(epoch_loss)\n",
    "            print(\"Epoch #{}\\tTime Elapsed: {}\\tLoss {}\".format(i, (time.time()-start)/60, epoch_loss))\n",
    "            \n",
    "        self.saver.save(self.sess, self.saved_model_location)\n",
    "        print(\"Model Saved\")\n",
    "        return self.losses\n",
    "    \n",
    "    def test(self):\n",
    "        feed_dict = {self.lstm_input: self.train_x[:32],\n",
    "                    self.labels_: self.train_z[:32]}\n",
    "        preds = self.sess.run([self.predictions], feed_dict=feed_dict)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = LSTM_model(sess)\n",
    "    model.build_LSTM().build_loss().initialize_variables().load_train_data()\n",
    "    losses = model.train(epochs = 1)\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = LSTM_model(sess)\n",
    "    model.build_LSTM().initialize_variables().load_train_data()\n",
    "    preds = model.test()\n",
    "    plt.plot(preds[0][0])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SSM]",
   "language": "python",
   "name": "conda-env-SSM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
